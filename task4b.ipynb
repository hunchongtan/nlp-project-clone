{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gf9JVYjNP8v"
   },
   "source": [
    "# Task 4b: Detecting Cultural Bias Across Subgroups Using UniBias\n",
    "\n",
    "This notebook uses UniBias to detect bias across identifiable subgroups (pillars) in the Total Defence Meme dataset.\n",
    "\n",
    "**Dataset**: SEACrowd/total_defense_meme\n",
    "- **Subgroups (Pillars)**: military, civil, economic, social, psychological, digital, others\n",
    "- **Task**: Stance classification (against, neutral, supportive)\n",
    "- **Goal**: Detect if model predictions are biased toward certain stances for certain pillars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10480,
     "status": "ok",
     "timestamp": 1764430857588,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "Io3_MYOdNP8w"
   },
   "outputs": [],
   "source": [
    "# Install required packages for UniBias adaptation experiment\n",
    "!pip install -q torch transformers datasets==2.19.1 seacrowd numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21957,
     "status": "ok",
     "timestamp": 1764430879551,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "qe1s_llENP8y",
    "outputId": "b7f49dcc-b8f4-4a9a-d0e2-e35bc3bd0849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2IQm9fJNP8y"
   },
   "source": [
    "## Step 1: Load Model and Dataset\n",
    "\n",
    "We'll use GPT-2 as our language model. GPT-2 uses **attention heads** - internal components that determine which parts of the input the model focuses on. We'll identify which of these heads contribute to bias.\n",
    "\n",
    "**Key Setup**:\n",
    "- Model: GPT-2 (124M parameters, 12 layers, 12 heads per layer = 144 total heads)\n",
    "- Attention Mode: `eager` (required to support `head_mask` for masking specific heads)\n",
    "- Device: CUDA (GPU) for faster computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1716,
     "status": "ok",
     "timestamp": 1764430881269,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "vP1JhEsRNP8z",
    "outputId": "10901385-92c0-4da4-e4d2-5f0a6274c3dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: gpt2\n",
      "Layers: 12, Heads per layer: 12\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", attn_implementation=\"eager\")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.config.name_or_path}\")\n",
    "print(f\"Layers: {model.config.n_layer}, Heads per layer: {model.config.n_head}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674,
     "referenced_widgets": [
      "c046f8c23c294c7b892af4c47f9a4bd4",
      "db20b4846da74816ab0200ff2266b9e0",
      "112ab4a81f0a47a4bbfce242f9e00b95",
      "829e0e0b096a4f72935dfe636bc03e06",
      "4f0711e5af1549ada63f9b2bfbbbbe3b",
      "ede39c58e797410fab84475756648370",
      "3a446921402c4497938b3ba9556b023e",
      "f21ab62168f440178c30cb9d0cb310f5",
      "c5e46ace07864489bbd5e7f85461a171",
      "932330cf200746d79a3b902c568ad21e",
      "275321de10444260afab1c77794dc44d",
      "406e5c96c1c6482f84fa52af02c18ca3",
      "17202f27177b463fa9113dbf0d2c4c14",
      "da42b769e9c64de2bbab7abb7de2447b",
      "f349749a5ba54b34ab5311471634d7dd",
      "e14d259844f0485e853af4e2cfdde6bd",
      "5482591f3a4f4db5a9a9fdd1528426a7",
      "3d4439826c08426a8e7d08b827e47cab",
      "c1575672ebb04753bb69e0cd533bebcb",
      "591f613cf9d34e01a59ff7f56d981bcc",
      "b88579fe4e0243239ac90b917b7ddfc9",
      "2925a382b1a04f5b988c815bd406e935"
     ]
    },
    "executionInfo": {
     "elapsed": 53718,
     "status": "ok",
     "timestamp": 1764430934994,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "OaC8mXGrNP8z",
    "outputId": "b51d0df4-ad48-4932-e0db-f04d1ef12fae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 89/402 [00:03<00:21, 14.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install the `pyreadr` package to use.\n",
      "Install the `pyreadr` package to use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 240/402 [00:08<00:04, 33.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install audiosegment to use the `national_speech_corpus_sg_imda` dataloader.\n",
      "Please install textgrid to use the `national_speech_corpus_sg_imda` dataloader.\n",
      "Please install audiosegment to use the `national_speech_corpus_sg_imda` dataloader.\n",
      "Please install textgrid to use the `national_speech_corpus_sg_imda` dataloader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 282/402 [00:09<00:03, 39.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install the `audiosegment` package to use.\n",
      "Install the `textgrid` package to use.\n",
      "Install the `audiosegment` package to use.\n",
      "Install the `textgrid` package to use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1486: FutureWarning: The repository for gem contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/gem\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1486: FutureWarning: The repository for GEM/xlsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/GEM/xlsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 402/402 [00:26<00:00, 15.11it/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1oJIh4QQS3Idff2g6bZORstS5uBROjUUz\n",
      "From (redirected): https://drive.google.com/uc?id=1oJIh4QQS3Idff2g6bZORstS5uBROjUUz&confirm=t&uuid=f3fdbb27-b911-48c0-814e-471af35e0aa9\n",
      "To: /content/data/total_defense_meme/total_defense_meme.zip\n",
      "100%|██████████| 767M/767M [00:11<00:00, 64.0MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c046f8c23c294c7b892af4c47f9a4bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406e5c96c1c6482f84fa52af02c18ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset splits: ['train']\n",
      "Train size: 7012\n",
      "\n",
      "First sample structure:\n",
      "{'id': '0', 'image_paths': ['/root/.cache/huggingface/datasets/downloads/extracted/f03b95ee59c393962bf7e1227024f7c3253d96d9937914cc57b444283a5cf2fb/TD_Memes/img_001.jpg'], 'texts': 'Types of accounts Your How contributions are allocated Ultimate CPF for the self-employed and contract staff Guide to Interest rates CPF The CPF LIFE Scheme Where your CPF money goes after you die PLANNERBEE', 'metadata': {'tags': None, 'pillar_stances': {'category': [], 'stance': []}}}\n",
      "\n",
      "Dataset features/columns:\n",
      "{'id': Value(dtype='string', id=None), 'image_paths': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'texts': Value(dtype='string', id=None), 'metadata': {'tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pillar_stances': Sequence(feature={'category': ClassLabel(names=['Social', 'Economic', 'Psychological', 'Military', 'Civil', 'Digital', 'Others'], id=None), 'stance': Sequence(feature=ClassLabel(names=['Against', 'Neutral', 'Supportive'], id=None), length=-1, id=None)}, length=-1, id=None)}}\n"
     ]
    }
   ],
   "source": [
    "import seacrowd as sc\n",
    "tdm = sc.load_dataset(\"total_defense_meme\", schema=\"seacrowd_imtext\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset splits: {list(tdm.keys())}\")\n",
    "print(f\"Train size: {len(tdm['train'])}\")\n",
    "print(\"\\nFirst sample structure:\")\n",
    "print(tdm[\"train\"][0])\n",
    "print(\"\\nDataset features/columns:\")\n",
    "print(tdm[\"train\"].features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-GnVk3pNP80"
   },
   "source": [
    "## Step 2: Prepare Data - Extract Text and Labels\n",
    "\n",
    "**Dataset**: Total Defence Meme dataset contains memes about Singapore's Total Defence policy.\n",
    "\n",
    "**What we extract**:\n",
    "- **Text**: OCR-extracted text from memes (from `texts` field)\n",
    "- **Pillar**: Subgroup category (military, civil, economic, social, psychological, digital, others)\n",
    "- **Stance**: Label (against, neutral, supportive)\n",
    "\n",
    "**Why subgroups matter**: We want to detect if the model shows **representational bias** - treating different pillars differently even when the content is similar. This is a form of bias where certain subgroups receive different predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1764431280240,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "ZdGy6X5wNP81",
    "outputId": "c5387a60-ef5a-47e2-8ae8-4984cedea351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2512 samples with text and labels\n",
      "\n",
      "Stance distribution:\n",
      "Counter({'neutral': 1137, 'against': 1079, 'supportive': 296})\n",
      "\n",
      "Pillar distribution:\n",
      "Counter({'others': 946, 'military': 724, 'psychological': 377, 'economic': 209, 'civil': 158, 'social': 59, 'digital': 39})\n",
      "\n",
      "Sample entries:\n",
      "  1. Pillar: economic, Stance: neutral\n",
      "     Text: When a HDB flat finishes it's 99-year lease: This is so sad can we hit a pedestrian with escooters?...\n",
      "  2. Pillar: economic, Stance: against\n",
      "     Text: when you've searched everywhere and still can't find your CPF This is what it means to be Singaporea...\n",
      "  3. Pillar: economic, Stance: against\n",
      "     Text: VOLUNTARY TOP UP MY CPF SPECIAL ACCOUNT??...\n",
      "\n",
      "Using first 50 samples for faster processing...\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_from_dataset(data):\n",
    "    \"\"\"Extract text, stance labels, and pillar labels from total_defense_meme (schema='seacrowd_imtext').\"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # Introspect label names from the dataset features\n",
    "    features = data.features\n",
    "    ps_feat = features[\"metadata\"][\"pillar_stances\"].feature\n",
    "\n",
    "    # ClassLabel objects for pillar category and stance\n",
    "    pillar_names = ps_feat[\"category\"].names\n",
    "    stance_names = ps_feat[\"stance\"].feature.names\n",
    "\n",
    "    for entry in data:\n",
    "        # 1. Get text (field is 'texts' for this dataset)\n",
    "        text = entry.get(\"texts\", None)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # 2. Get pillar_stances from metadata\n",
    "        metadata = entry.get(\"metadata\", {})\n",
    "        pillar_stances = metadata.get(\"pillar_stances\", {})\n",
    "\n",
    "        categories = pillar_stances.get(\"category\") or []\n",
    "        stances = pillar_stances.get(\"stance\") or []\n",
    "\n",
    "        # Skip examples with no labels\n",
    "        if not categories or not stances:\n",
    "            continue\n",
    "\n",
    "        # 3. Decode pillar (take the first category index)\n",
    "        cat_idx = categories[0]\n",
    "        if not isinstance(cat_idx, int) or not (0 <= cat_idx < len(pillar_names)):\n",
    "            continue\n",
    "        pillar = pillar_names[cat_idx]\n",
    "\n",
    "        # 4. Decode stance (handle possible nested list structure)\n",
    "        first_stance = stances[0]\n",
    "        if isinstance(first_stance, list):\n",
    "            if not first_stance:\n",
    "                continue\n",
    "            stance_idx = first_stance[0]\n",
    "        else:\n",
    "            stance_idx = first_stance\n",
    "\n",
    "        if not isinstance(stance_idx, int) or not (0 <= stance_idx < len(stance_names)):\n",
    "            continue\n",
    "        stance = stance_names[stance_idx]\n",
    "\n",
    "        # 5. Append clean sample\n",
    "        samples.append({\n",
    "            \"text\": str(text),\n",
    "            \"stance\": stance.lower(),\n",
    "            \"pillar\": pillar.lower(),\n",
    "        })\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Use the loaded dataset\n",
    "if 'tdm' not in locals() or tdm is None or 'train' not in tdm:\n",
    "    raise ValueError(\"Dataset 'tdm' not loaded. Please run the previous cell to load the dataset first.\")\n",
    "\n",
    "data = tdm['train']\n",
    "samples = prepare_data_from_dataset(data)\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise ValueError(\"No samples extracted from dataset. Please check the dataset structure and field extraction logic.\")\n",
    "\n",
    "print(f\"Prepared {len(samples)} samples with text and labels\")\n",
    "\n",
    "# Analyze distribution\n",
    "from collections import Counter\n",
    "stances = [s['stance'] for s in samples]\n",
    "pillars = [s['pillar'] for s in samples]\n",
    "\n",
    "print(f\"\\nStance distribution:\")\n",
    "print(Counter(stances))\n",
    "\n",
    "print(f\"\\nPillar distribution:\")\n",
    "print(Counter(pillars))\n",
    "\n",
    "print(f\"\\nSample entries:\")\n",
    "for i, s in enumerate(samples[:3]):\n",
    "    print(f\"  {i+1}. Pillar: {s['pillar']}, Stance: {s['stance']}\")\n",
    "    print(f\"     Text: {s['text'][:100]}...\")\n",
    "\n",
    "# Use a sample for faster processing\n",
    "if len(samples) > 50:\n",
    "    print(f\"\\nUsing first 50 samples for faster processing...\")\n",
    "    samples = samples[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Prompts and Define Answer Tokens\n",
    "\n",
    "**Task Design**: We frame this as a **stance classification** task using few-shot prompting. This is compatible with UniBias methodology, which works best with discrete outputs (classification tasks).\n",
    "\n",
    "**Prompt Format**: Few-shot examples showing the model how to classify stance, followed by the meme text.\n",
    "\n",
    "**Answer Tokens**: We need to find which token IDs correspond to each stance label (against, neutral, supportive). UniBias measures bias by examining logit differences between these tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzaBg1qlNP81"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1764431285917,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "9OSOyyJNNP82",
    "outputId": "28c4eb4d-e299-4c7d-9356-02855aec1707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50 prompts\n",
      "\n",
      "Example prompt:\n",
      "Classify the stance toward Total Defence:\n",
      "Text: Total Defence is essential for Singapore's security\n",
      "Stance: supportive\n",
      "\n",
      "Text: I disagree with Total Defence policies\n",
      "Stance: against\n",
      "\n",
      "Text: Total Defenc...\n"
     ]
    }
   ],
   "source": [
    "# Create prompts for stance classification\n",
    "def create_stance_prompt(text):\n",
    "    \"\"\"Create a prompt for stance classification\"\"\"\n",
    "    # Few-shot examples\n",
    "    demonstration = \"\"\"Classify the stance toward Total Defence:\n",
    "Text: Total Defence is essential for Singapore's security\n",
    "Stance: supportive\n",
    "\n",
    "Text: I disagree with Total Defence policies\n",
    "Stance: against\n",
    "\n",
    "Text: Total Defence has both pros and cons\n",
    "Stance: neutral\n",
    "\n",
    "\"\"\"\n",
    "    return demonstration + f\"Text: {text}\\nStance:\"\n",
    "\n",
    "# Create prompts\n",
    "prompts = [create_stance_prompt(s['text']) for s in samples]\n",
    "labels = [s['stance'] for s in samples]\n",
    "pillars = [s['pillar'] for s in samples]\n",
    "\n",
    "print(f\"Created {len(prompts)} prompts\")\n",
    "print(f\"\\nExample prompt:\\n{prompts[0][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1764431289119,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "IL1xX_kaNP82",
    "outputId": "2aebd87c-ccae-4d5c-b65f-395fe6774299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs found:\n",
      "  against: 1 token IDs (showing first 5: [32826])\n",
      "  neutral: 1 token IDs (showing first 5: [29797])\n",
      "  supportive: 2 token IDs (showing first 5: [11284, 425])\n"
     ]
    }
   ],
   "source": [
    "# Answer tokens for stance classification\n",
    "ans_token_list = [['against'], ['neutral'], ['supportive']]\n",
    "\n",
    "def find_possible_ids_for_labels(ans_token_list, tokenizer):\n",
    "    \"\"\"Find all possible token IDs for each label\"\"\"\n",
    "    ids_dict = {}\n",
    "    for ans_tuple in ans_token_list:\n",
    "        label = ans_tuple[0]\n",
    "        ids_dict[label] = []\n",
    "        # Try encoding the label directly\n",
    "        encoded = tokenizer.encode(label, add_special_tokens=False)\n",
    "        if encoded:\n",
    "            ids_dict[label].extend(encoded)\n",
    "        # Also search for tokens that contain the label\n",
    "        for token_id in range(min(1000, len(tokenizer))):  # Limit search for speed\n",
    "            try:\n",
    "                decoded = tokenizer.decode([token_id])\n",
    "                if label.lower() in decoded.lower() and len(decoded) > 0:\n",
    "                    if token_id not in ids_dict[label]:\n",
    "                        ids_dict[label].append(token_id)\n",
    "            except:\n",
    "                continue\n",
    "    return ids_dict\n",
    "\n",
    "gt_ans_ids_dict = find_possible_ids_for_labels(ans_token_list, tokenizer)\n",
    "print(f\"Token IDs found:\")\n",
    "for label, ids in gt_ans_ids_dict.items():\n",
    "    print(f\"  {label}: {len(ids)} token IDs (showing first 5: {ids[:5]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Urkj60pYNP83"
   },
   "source": [
    "## Step 4: Baseline Evaluation - Measure Predictions by Pillar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup Analysis Results\n",
    "\n",
    "This analysis reveals if the model shows **representational bias** - different prediction patterns across different pillars (subgroups). \n",
    "\n",
    "**What to look for**:\n",
    "- **Different favored stances**: Do some pillars favor \"against\" while others favor \"neutral\"?\n",
    "- **Different bias magnitudes**: Do some pillars show stronger bias (larger logit differences) than others?\n",
    "- **Accuracy differences**: Does the model perform better on some pillars than others?\n",
    "\n",
    "If we see systematic differences, this indicates **representational bias** - the model is treating subgroups differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1885,
     "status": "ok",
     "timestamp": 1764431292657,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "xWSaHtptNP83",
    "outputId": "ead3fbfd-aaab-412c-a872-e5b08c1ced4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing predictions: 100%|██████████| 50/50 [00:01<00:00, 26.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Predictions:\n",
      "  Against: 19 (38.0%)\n",
      "  Neutral: 29 (58.0%)\n",
      "  Supportive: 2 (4.0%)\n",
      "\n",
      "Average Logits by Stance:\n",
      "  against: -109.0636\n",
      "  neutral: -108.7679\n",
      "  supportive: -109.8780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_predictions_by_pillar(model, tokenizer, prompts, labels, pillars, gt_ans_ids_dict):\n",
    "    \"\"\"Compute predictions and analyze by pillar\"\"\"\n",
    "    predictions = []\n",
    "    logits_by_stance = {'against': [], 'neutral': [], 'supportive': []}\n",
    "    results_by_pillar = defaultdict(lambda: {'predictions': [], 'labels': [], 'logits': {'against': [], 'neutral': [], 'supportive': []}})\n",
    "\n",
    "    for i, prompt in enumerate(tqdm(prompts, desc=\"Computing predictions\")):\n",
    "        encoded = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        pillar = pillars[i]\n",
    "        true_label = labels[i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            logits = outputs.logits[0, -1]  # Last token logits\n",
    "\n",
    "            # Get logits for each stance\n",
    "            stance_logits = {}\n",
    "            for stance in ['against', 'neutral', 'supportive']:\n",
    "                stance_ids = gt_ans_ids_dict[stance]\n",
    "                if stance_ids:\n",
    "                    stance_logit = max([logits[id].item() for id in stance_ids if id < len(logits)], default=-100)\n",
    "                else:\n",
    "                    stance_logit = -100\n",
    "                stance_logits[stance] = stance_logit\n",
    "                logits_by_stance[stance].append(stance_logit)\n",
    "                results_by_pillar[pillar]['logits'][stance].append(stance_logit)\n",
    "\n",
    "            # Prediction: highest logit\n",
    "            predicted_stance = max(stance_logits.items(), key=lambda x: x[1])[0]\n",
    "            predictions.append(predicted_stance)\n",
    "            results_by_pillar[pillar]['predictions'].append(predicted_stance)\n",
    "            results_by_pillar[pillar]['labels'].append(true_label)\n",
    "\n",
    "    return predictions, logits_by_stance, results_by_pillar\n",
    "\n",
    "predictions, logits_by_stance, results_by_pillar = compute_predictions_by_pillar(\n",
    "    model, tokenizer, prompts, labels, pillars, gt_ans_ids_dict\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall Predictions:\")\n",
    "print(f\"  Against: {predictions.count('against')} ({predictions.count('against')/len(predictions)*100:.1f}%)\")\n",
    "print(f\"  Neutral: {predictions.count('neutral')} ({predictions.count('neutral')/len(predictions)*100:.1f}%)\")\n",
    "print(f\"  Supportive: {predictions.count('supportive')} ({predictions.count('supportive')/len(predictions)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAverage Logits by Stance:\")\n",
    "for stance, logit_list in logits_by_stance.items():\n",
    "    if logit_list:\n",
    "        print(f\"  {stance}: {np.mean(logit_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1764431294551,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "fbhsz6yrNP84",
    "outputId": "f98c13e1-0a0c-452b-ad84-eb79998d011f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREDICTIONS BY PILLAR (Subgroup Analysis)\n",
      "================================================================================\n",
      "\n",
      "Pillar: CIVIL\n",
      "  Samples: 4\n",
      "  Predictions: {'neutral': 2, 'against': 2}\n",
      "  True Labels: {'neutral': 2, 'against': 1, 'supportive': 1}\n",
      "  Avg Logits: {'against': np.float64(-111.74154472351074), 'neutral': np.float64(-111.31406593322754), 'supportive': np.float64(-112.71752738952637)}\n",
      "  Bias: Favors 'neutral' (magnitude: 1.4035)\n",
      "\n",
      "Pillar: DIGITAL\n",
      "  Samples: 5\n",
      "  Predictions: {'neutral': 4, 'against': 1}\n",
      "  True Labels: {'against': 5}\n",
      "  Avg Logits: {'against': np.float64(-108.34479370117188), 'neutral': np.float64(-108.32415466308593), 'supportive': np.float64(-109.67637939453125)}\n",
      "  Bias: Favors 'neutral' (magnitude: 1.3522)\n",
      "\n",
      "Pillar: ECONOMIC\n",
      "  Samples: 16\n",
      "  Predictions: {'against': 8, 'neutral': 6, 'supportive': 2}\n",
      "  True Labels: {'neutral': 3, 'against': 11, 'supportive': 2}\n",
      "  Avg Logits: {'against': np.float64(-110.32508373260498), 'neutral': np.float64(-110.53112506866455), 'supportive': np.float64(-110.97400045394897)}\n",
      "  Bias: Favors 'against' (magnitude: 0.6489)\n",
      "\n",
      "Pillar: MILITARY\n",
      "  Samples: 15\n",
      "  Predictions: {'against': 4, 'neutral': 11}\n",
      "  True Labels: {'neutral': 9, 'against': 5, 'supportive': 1}\n",
      "  Avg Logits: {'against': np.float64(-107.59446512858072), 'neutral': np.float64(-106.77720947265625), 'supportive': np.float64(-108.40392150878907)}\n",
      "  Bias: Favors 'neutral' (magnitude: 1.6267)\n",
      "\n",
      "Pillar: OTHERS\n",
      "  Samples: 3\n",
      "  Predictions: {'neutral': 3}\n",
      "  True Labels: {'against': 2, 'neutral': 1}\n",
      "  Avg Logits: {'against': np.float64(-106.23587544759114), 'neutral': np.float64(-105.64039103190105), 'supportive': np.float64(-106.80849202473958)}\n",
      "  Bias: Favors 'neutral' (magnitude: 1.1681)\n",
      "\n",
      "Pillar: PSYCHOLOGICAL\n",
      "  Samples: 5\n",
      "  Predictions: {'neutral': 1, 'against': 4}\n",
      "  True Labels: {'supportive': 2, 'against': 2, 'neutral': 1}\n",
      "  Avg Logits: {'against': np.float64(-110.4890853881836), 'neutral': np.float64(-110.52995147705079), 'supportive': np.float64(-111.28864440917968)}\n",
      "  Bias: Favors 'against' (magnitude: 0.7996)\n",
      "\n",
      "Pillar: SOCIAL\n",
      "  Samples: 2\n",
      "  Predictions: {'neutral': 2}\n",
      "  True Labels: {'against': 1, 'neutral': 1}\n",
      "  Avg Logits: {'against': np.float64(-107.10834121704102), 'neutral': np.float64(-105.89623641967773), 'supportive': np.float64(-108.06771469116211)}\n",
      "  Bias: Favors 'neutral' (magnitude: 2.1715)\n"
     ]
    }
   ],
   "source": [
    "# Analyze predictions by pillar\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTIONS BY PILLAR (Subgroup Analysis)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for pillar in sorted(results_by_pillar.keys()):\n",
    "    pillar_data = results_by_pillar[pillar]\n",
    "    if len(pillar_data['predictions']) == 0:\n",
    "        continue\n",
    "\n",
    "    pred_counts = Counter(pillar_data['predictions'])\n",
    "    label_counts = Counter(pillar_data['labels'])\n",
    "\n",
    "    print(f\"\\nPillar: {pillar.upper()}\")\n",
    "    print(f\"  Samples: {len(pillar_data['predictions'])}\")\n",
    "    print(f\"  Predictions: {dict(pred_counts)}\")\n",
    "    print(f\"  True Labels: {dict(label_counts)}\")\n",
    "\n",
    "    # Calculate average logits for this pillar\n",
    "    avg_logits = {}\n",
    "    for stance in ['against', 'neutral', 'supportive']:\n",
    "        if pillar_data['logits'][stance]:\n",
    "            avg_logits[stance] = np.mean(pillar_data['logits'][stance])\n",
    "        else:\n",
    "            avg_logits[stance] = -100\n",
    "\n",
    "    print(f\"  Avg Logits: {avg_logits}\")\n",
    "\n",
    "    # Calculate bias: difference between highest and lowest logit\n",
    "    if avg_logits:\n",
    "        max_logit = max(avg_logits.values())\n",
    "        min_logit = min(avg_logits.values())\n",
    "        bias_magnitude = max_logit - min_logit\n",
    "        favored_stance = max(avg_logits.items(), key=lambda x: x[1])[0]\n",
    "        print(f\"  Bias: Favors '{favored_stance}' (magnitude: {bias_magnitude:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Bias Score\n",
    "\n",
    "This measures the overall bias in the model's stance predictions. Higher standard deviation means the model strongly favors one stance over others.\n",
    "\n",
    "**Interpretation**:\n",
    "- High bias score = model strongly favors one stance (e.g., always predicts \"neutral\")\n",
    "- Low bias score = model is more balanced across stances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT-9qyBnNP84"
   },
   "source": [
    "## Step 5: Detect Bias Using UniBias - Identify Biased Heads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Identification Process\n",
    "\n",
    "**What we're doing**: Testing each attention head to see if masking it reduces bias.\n",
    "\n",
    "**For each head**:\n",
    "- Create a mask that zeros out only that specific head\n",
    "- Run all prompts through the model with this mask\n",
    "- Measure how much bias changes (delta)\n",
    "\n",
    "**Delta interpretation**:\n",
    "- **Positive delta** (e.g., 0.158): Masking this head reduces bias → this head contributes to bias\n",
    "- **Negative delta** (e.g., -0.028): Masking this head increases bias → this head may be reducing bias\n",
    "\n",
    "**Note**: We analyze first 2 layers (24 heads) for speed. Full analysis would check all 144 heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2106,
     "status": "ok",
     "timestamp": 1764431302516,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "9WXBzk6WNP84",
    "outputId": "f1897dd5-8bda-44d9-b8ee-85f3e6ef92da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Bias Score (std of avg logits): 0.4694\n",
      "Average Logits: {'against': np.float64(-109.06356399536133), 'neutral': np.float64(-108.76793167114258), 'supportive': np.float64(-109.87797927856445)}\n"
     ]
    }
   ],
   "source": [
    "def compute_baseline_bias(model, tokenizer, prompts, gt_ans_ids_dict):\n",
    "    \"\"\"Compute baseline bias across all stances\"\"\"\n",
    "    stance_logits = {'against': [], 'neutral': [], 'supportive': []}\n",
    "\n",
    "    for prompt in prompts:\n",
    "        encoded = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            logits = outputs.logits[0, -1]\n",
    "\n",
    "            for stance in ['against', 'neutral', 'supportive']:\n",
    "                stance_ids = gt_ans_ids_dict[stance]\n",
    "                if stance_ids:\n",
    "                    stance_logit = max([logits[id].item() for id in stance_ids if id < len(logits)], default=-100)\n",
    "                    stance_logits[stance].append(stance_logit)\n",
    "\n",
    "    # Bias = variance in average logits (higher variance = more bias)\n",
    "    avg_logits = {stance: np.mean(logits) for stance, logits in stance_logits.items() if logits}\n",
    "    bias_score = np.std(list(avg_logits.values()))  # Standard deviation measures bias\n",
    "\n",
    "    return bias_score, avg_logits\n",
    "\n",
    "baseline_bias, baseline_avg_logits = compute_baseline_bias(model, tokenizer, prompts, gt_ans_ids_dict)\n",
    "print(f\"Baseline Bias Score (std of avg logits): {baseline_bias:.4f}\")\n",
    "print(f\"Average Logits: {baseline_avg_logits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Biased Heads\n",
    "\n",
    "**Top 3 heads selected for masking**:\n",
    "- These are the heads that, when masked, show the largest bias reduction\n",
    "- Layer 0, Head 5 shows the strongest effect (delta: 0.1588)\n",
    "- Early layers (0-1) contain the most biased heads\n",
    "\n",
    "**Why mask these**: By removing these heads' contributions, we should reduce the model's bias toward certain stances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19196,
     "status": "ok",
     "timestamp": 1764431323270,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "-VWUf9fZNP85",
    "outputId": "2eaa0a3d-f362-434a-e158-1f50dbe66729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying biased attention heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layers: 100%|██████████| 2/2 [00:19<00:00,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzed 24 attention heads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_head_contribution(model, tokenizer, prompts, gt_ans_ids_dict, layer_idx, head_idx, baseline_bias):\n",
    "    \"\"\"Compute how much a specific head contributes to bias\"\"\"\n",
    "    num_layers = model.config.n_layer\n",
    "    num_heads = model.config.n_head\n",
    "\n",
    "    # Create head mask: mask this specific head\n",
    "    head_mask = torch.ones(num_layers, num_heads, device=DEVICE)\n",
    "    head_mask[layer_idx, head_idx] = 0.0\n",
    "\n",
    "    stance_logits = {'against': [], 'neutral': [], 'supportive': []}\n",
    "\n",
    "    for prompt in prompts:\n",
    "        encoded = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded, head_mask=head_mask)\n",
    "            logits = outputs.logits[0, -1]\n",
    "\n",
    "            for stance in ['against', 'neutral', 'supportive']:\n",
    "                stance_ids = gt_ans_ids_dict[stance]\n",
    "                if stance_ids:\n",
    "                    stance_logit = max([logits[id].item() for id in stance_ids if id < len(logits)], default=-100)\n",
    "                    stance_logits[stance].append(stance_logit)\n",
    "\n",
    "    avg_logits = {stance: np.mean(logits) for stance, logits in stance_logits.items() if logits}\n",
    "    masked_bias = np.std(list(avg_logits.values()))\n",
    "\n",
    "    # Delta: how much bias changes when this head is masked\n",
    "    delta = baseline_bias - masked_bias\n",
    "\n",
    "    return delta, masked_bias\n",
    "\n",
    "print(\"Identifying biased attention heads...\")\n",
    "head_contributions = {}\n",
    "\n",
    "num_layers = model.config.n_layer\n",
    "num_heads = model.config.n_head\n",
    "\n",
    "# Sample subset for speed (analyze first 2 layers)\n",
    "SAMPLE_HEADS = True\n",
    "if SAMPLE_HEADS:\n",
    "    layers_to_check = [0, 1]\n",
    "else:\n",
    "    layers_to_check = range(num_layers)\n",
    "\n",
    "for layer_idx in tqdm(layers_to_check, desc=\"Layers\"):\n",
    "    for head_idx in range(num_heads):\n",
    "        delta, masked_bias = compute_head_contribution(\n",
    "            model, tokenizer, prompts[:min(20, len(prompts))],  # Use subset for speed\n",
    "            gt_ans_ids_dict, layer_idx, head_idx, baseline_bias\n",
    "        )\n",
    "        head_contributions[(layer_idx, head_idx)] = {\n",
    "            'delta': delta,\n",
    "            'masked_bias': masked_bias\n",
    "        }\n",
    "\n",
    "print(f\"\\nAnalyzed {len(head_contributions)} attention heads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1764431323299,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "NA5BjDb1NP85",
    "outputId": "3bf5e665-45eb-4854-84f9-3ba009691c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 heads by bias reduction (when masked):\n",
      "Layer 0, Head 5: delta=0.158836, masked_bias=0.3105\n",
      "Layer 0, Head 11: delta=0.038258, masked_bias=0.4311\n",
      "Layer 1, Head 2: delta=0.035952, masked_bias=0.4334\n",
      "Layer 1, Head 10: delta=0.029752, masked_bias=0.4396\n",
      "Layer 1, Head 4: delta=0.012714, masked_bias=0.4567\n",
      "Layer 1, Head 3: delta=-0.005042, masked_bias=0.4744\n",
      "Layer 1, Head 0: delta=-0.015707, masked_bias=0.4851\n",
      "Layer 1, Head 9: delta=-0.025497, masked_bias=0.4949\n",
      "Layer 0, Head 7: delta=-0.028404, masked_bias=0.4978\n",
      "Layer 1, Head 11: delta=-0.040065, masked_bias=0.5094\n",
      "\n",
      "Selected top 3 biased heads for masking: [(0, 5), (0, 11), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Find top biased heads\n",
    "sorted_heads = sorted(head_contributions.items(), key=lambda x: x[1]['delta'], reverse=True)\n",
    "\n",
    "print(\"Top 10 heads by bias reduction (when masked):\")\n",
    "for (layer, head), stats in sorted_heads[:10]:\n",
    "    print(f\"Layer {layer}, Head {head}: delta={stats['delta']:.6f}, masked_bias={stats['masked_bias']:.4f}\")\n",
    "\n",
    "# Select top 3 heads for masking\n",
    "top_biased_heads = [layer_head for layer_head, _ in sorted_heads[:3]]\n",
    "print(f\"\\nSelected top 3 biased heads for masking: {top_biased_heads}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Head Mask\n",
    "\n",
    "We create a mask that zeros out the selected biased heads while keeping all other heads active. This allows us to test if removing these heads' contributions reduces bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h23z3QBhNP85"
   },
   "source": [
    "## Step 6: Mask Biased Heads and Re-analyze by Pillar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results After Masking\n",
    "\n",
    "**Key Changes**:\n",
    "- **Supportive predictions increased**: From 4% to 16% (4x increase!)\n",
    "- **Neutral predictions decreased**: From 58% to 48%\n",
    "- **Against predictions decreased**: From 38% to 36%\n",
    "\n",
    "**Interpretation**: Masking biased heads successfully reduced the model's strong bias toward \"neutral\" stance, making predictions more balanced. The model now predicts \"supportive\" more often, which was previously under-represented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1764431323323,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "DnwduZHmNP85",
    "outputId": "f9f23805-cb8d-4521-cdc7-00c95ad19dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking Layer 0, Head 5\n",
      "Masking Layer 0, Head 11\n",
      "Masking Layer 1, Head 2\n",
      "\n",
      "Masked 3 attention heads\n"
     ]
    }
   ],
   "source": [
    "# Create head mask for top biased heads\n",
    "num_layers = model.config.n_layer\n",
    "num_heads = model.config.n_head\n",
    "head_mask = torch.ones(num_layers, num_heads, device=DEVICE)\n",
    "\n",
    "for layer_idx, head_idx in top_biased_heads:\n",
    "    head_mask[layer_idx, head_idx] = 0.0\n",
    "    print(f\"Masking Layer {layer_idx}, Head {head_idx}\")\n",
    "\n",
    "print(f\"\\nMasked {len(top_biased_heads)} attention heads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1653,
     "status": "ok",
     "timestamp": 1764431324980,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "K4KOFFukNP86",
    "outputId": "ec28f79f-d04e-41f6-8df0-cb1227a03498"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating with mask: 100%|██████████| 50/50 [00:01<00:00, 30.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions After Masking:\n",
      "  Against: 18 (36.0%)\n",
      "  Neutral: 24 (48.0%)\n",
      "  Supportive: 8 (16.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-analyze predictions by pillar after masking\n",
    "def compute_predictions_by_pillar_masked(model, tokenizer, prompts, labels, pillars, gt_ans_ids_dict, head_mask):\n",
    "    \"\"\"Compute predictions with head mask\"\"\"\n",
    "    predictions = []\n",
    "    results_by_pillar = defaultdict(lambda: {'predictions': [], 'labels': [], 'logits': {'against': [], 'neutral': [], 'supportive': []}})\n",
    "\n",
    "    for i, prompt in enumerate(tqdm(prompts, desc=\"Evaluating with mask\")):\n",
    "        encoded = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        pillar = pillars[i]\n",
    "        true_label = labels[i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded, head_mask=head_mask)\n",
    "            logits = outputs.logits[0, -1]\n",
    "\n",
    "            stance_logits = {}\n",
    "            for stance in ['against', 'neutral', 'supportive']:\n",
    "                stance_ids = gt_ans_ids_dict[stance]\n",
    "                if stance_ids:\n",
    "                    stance_logit = max([logits[id].item() for id in stance_ids if id < len(logits)], default=-100)\n",
    "                else:\n",
    "                    stance_logit = -100\n",
    "                stance_logits[stance] = stance_logit\n",
    "                results_by_pillar[pillar]['logits'][stance].append(stance_logit)\n",
    "\n",
    "            predicted_stance = max(stance_logits.items(), key=lambda x: x[1])[0]\n",
    "            predictions.append(predicted_stance)\n",
    "            results_by_pillar[pillar]['predictions'].append(predicted_stance)\n",
    "            results_by_pillar[pillar]['labels'].append(true_label)\n",
    "\n",
    "    return predictions, results_by_pillar\n",
    "\n",
    "masked_predictions, masked_results_by_pillar = compute_predictions_by_pillar_masked(\n",
    "    model, tokenizer, prompts, labels, pillars, gt_ans_ids_dict, head_mask\n",
    ")\n",
    "\n",
    "print(f\"\\nPredictions After Masking:\")\n",
    "print(f\"  Against: {masked_predictions.count('against')} ({masked_predictions.count('against')/len(masked_predictions)*100:.1f}%)\")\n",
    "print(f\"  Neutral: {masked_predictions.count('neutral')} ({masked_predictions.count('neutral')/len(masked_predictions)*100:.1f}%)\")\n",
    "print(f\"  Supportive: {masked_predictions.count('supportive')} ({masked_predictions.count('supportive')/len(masked_predictions)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA-urx1hNP86"
   },
   "source": [
    "## Step 7: Compare Bias Across Pillars - Before vs After\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Interpretation\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Representational Bias Detected**: Model predictions systematically differ across pillars\n",
    "   - Economic & Psychological pillars favor \"against\" stance\n",
    "   - All other pillars favor \"neutral\" stance\n",
    "   - Social pillar shows highest bias magnitude (2.17)\n",
    "\n",
    "2. **Head Identification Successful**: Identified 3 biased attention heads\n",
    "   - Layer 0, Head 5 shows strongest effect (delta: 0.1588)\n",
    "   - Early layers (0-1) contain most biased heads\n",
    "\n",
    "3. **Mitigation Partially Effective**: \n",
    "   - 5 out of 7 pillars showed bias reduction ✅\n",
    "   - 2 pillars (Digital, Psychological) showed increased bias ❌\n",
    "   - Supportive predictions increased 4x (from 4% to 16%)\n",
    "\n",
    "4. **Subgroup-Specific Effects**: Different pillars respond differently to masking, suggesting complex bias mechanisms that may require pillar-specific mitigation strategies.\n",
    "\n",
    "### Implications\n",
    "\n",
    "- **Subgroup analysis is essential**: Overall bias metrics can mask differential treatment of subgroups\n",
    "- **Mitigation strategies should be subgroup-aware**: One-size-fits-all may not work\n",
    "- **Complete masking may be too aggressive**: Some heads may have mixed effects (reducing bias for some subgroups, increasing for others)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1764431325036,
     "user": {
      "displayName": "Hun Chong Tan",
      "userId": "17087738823364615848"
     },
     "user_tz": -480
    },
    "id": "ETKu7eHqNP86",
    "outputId": "56941efb-9e86-464b-8b0c-9bf4cd72e1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BIAS ANALYSIS BY PILLAR: Baseline vs After Masking\n",
      "================================================================================\n",
      "\n",
      "Pillar: CIVIL\n",
      "  Samples: 4\n",
      "  Baseline: Favors 'neutral' (bias: 1.4035)\n",
      "  After Masking: Favors 'neutral' (bias: 0.6006)\n",
      "  Bias Reduction: +0.8029\n",
      "\n",
      "Pillar: DIGITAL\n",
      "  Samples: 5\n",
      "  Baseline: Favors 'neutral' (bias: 1.3522)\n",
      "  After Masking: Favors 'neutral' (bias: 1.6411)\n",
      "  Bias Reduction: -0.2889\n",
      "\n",
      "Pillar: ECONOMIC\n",
      "  Samples: 16\n",
      "  Baseline: Favors 'against' (bias: 0.6489)\n",
      "  After Masking: Favors 'against' (bias: 0.2547)\n",
      "  Bias Reduction: +0.3942\n",
      "\n",
      "Pillar: MILITARY\n",
      "  Samples: 15\n",
      "  Baseline: Favors 'neutral' (bias: 1.6267)\n",
      "  After Masking: Favors 'neutral' (bias: 1.5039)\n",
      "  Bias Reduction: +0.1228\n",
      "\n",
      "Pillar: OTHERS\n",
      "  Samples: 3\n",
      "  Baseline: Favors 'neutral' (bias: 1.1681)\n",
      "  After Masking: Favors 'neutral' (bias: 0.8433)\n",
      "  Bias Reduction: +0.3248\n",
      "\n",
      "Pillar: PSYCHOLOGICAL\n",
      "  Samples: 5\n",
      "  Baseline: Favors 'against' (bias: 0.7996)\n",
      "  After Masking: Favors 'supportive' (bias: 0.8341)\n",
      "  Bias Reduction: -0.0345\n",
      "\n",
      "Pillar: SOCIAL\n",
      "  Samples: 2\n",
      "  Baseline: Favors 'neutral' (bias: 2.1715)\n",
      "  After Masking: Favors 'neutral' (bias: 1.8514)\n",
      "  Bias Reduction: +0.3200\n",
      "\n",
      "================================================================================\n",
      "SUMMARY TABLE\n",
      "================================================================================\n",
      "       Pillar  Samples Baseline_Favored Baseline_Bias Masked_Favored Masked_Bias Bias_Reduction\n",
      "        civil        4          neutral        1.4035        neutral      0.6006         0.8029\n",
      "      digital        5          neutral        1.3522        neutral      1.6411        -0.2889\n",
      "     economic       16          against        0.6489        against      0.2547         0.3942\n",
      "     military       15          neutral        1.6267        neutral      1.5039         0.1228\n",
      "       others        3          neutral        1.1681        neutral      0.8433         0.3248\n",
      "psychological        5          against        0.7996     supportive      0.8341        -0.0345\n",
      "       social        2          neutral        2.1715        neutral      1.8514         0.3200\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BIAS ANALYSIS BY PILLAR: Baseline vs After Masking\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for pillar in sorted(set(pillars)):\n",
    "    if pillar not in results_by_pillar or len(results_by_pillar[pillar]['predictions']) == 0:\n",
    "        continue\n",
    "\n",
    "    baseline_data = results_by_pillar[pillar]\n",
    "    masked_data = masked_results_by_pillar[pillar]\n",
    "\n",
    "    # Calculate bias for baseline\n",
    "    baseline_avg_logits = {}\n",
    "    for stance in ['against', 'neutral', 'supportive']:\n",
    "        if baseline_data['logits'][stance]:\n",
    "            baseline_avg_logits[stance] = np.mean(baseline_data['logits'][stance])\n",
    "        else:\n",
    "            baseline_avg_logits[stance] = -100\n",
    "\n",
    "    baseline_bias_mag = max(baseline_avg_logits.values()) - min(baseline_avg_logits.values())\n",
    "    baseline_favored = max(baseline_avg_logits.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    # Calculate bias for masked\n",
    "    masked_avg_logits = {}\n",
    "    for stance in ['against', 'neutral', 'supportive']:\n",
    "        if masked_data['logits'][stance]:\n",
    "            masked_avg_logits[stance] = np.mean(masked_data['logits'][stance])\n",
    "        else:\n",
    "            masked_avg_logits[stance] = -100\n",
    "\n",
    "    masked_bias_mag = max(masked_avg_logits.values()) - min(masked_avg_logits.values())\n",
    "    masked_favored = max(masked_avg_logits.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    bias_reduction = baseline_bias_mag - masked_bias_mag\n",
    "\n",
    "    comparison_data.append({\n",
    "        'Pillar': pillar,\n",
    "        'Samples': len(baseline_data['predictions']),\n",
    "        'Baseline_Favored': baseline_favored,\n",
    "        'Baseline_Bias': f\"{baseline_bias_mag:.4f}\",\n",
    "        'Masked_Favored': masked_favored,\n",
    "        'Masked_Bias': f\"{masked_bias_mag:.4f}\",\n",
    "        'Bias_Reduction': f\"{bias_reduction:.4f}\"\n",
    "    })\n",
    "\n",
    "    print(f\"\\nPillar: {pillar.upper()}\")\n",
    "    print(f\"  Samples: {len(baseline_data['predictions'])}\")\n",
    "    print(f\"  Baseline: Favors '{baseline_favored}' (bias: {baseline_bias_mag:.4f})\")\n",
    "    print(f\"  After Masking: Favors '{masked_favored}' (bias: {masked_bias_mag:.4f})\")\n",
    "    print(f\"  Bias Reduction: {bias_reduction:+.4f}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "if comparison_data:\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OISl66GxNP86"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "112ab4a81f0a47a4bbfce242f9e00b95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f21ab62168f440178c30cb9d0cb310f5",
      "max": 1446853,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5e46ace07864489bbd5e7f85461a171",
      "value": 1446853
     }
    },
    "17202f27177b463fa9113dbf0d2c4c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5482591f3a4f4db5a9a9fdd1528426a7",
      "placeholder": "​",
      "style": "IPY_MODEL_3d4439826c08426a8e7d08b827e47cab",
      "value": "Generating train split: "
     }
    },
    "275321de10444260afab1c77794dc44d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2925a382b1a04f5b988c815bd406e935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a446921402c4497938b3ba9556b023e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d4439826c08426a8e7d08b827e47cab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "406e5c96c1c6482f84fa52af02c18ca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17202f27177b463fa9113dbf0d2c4c14",
       "IPY_MODEL_da42b769e9c64de2bbab7abb7de2447b",
       "IPY_MODEL_f349749a5ba54b34ab5311471634d7dd"
      ],
      "layout": "IPY_MODEL_e14d259844f0485e853af4e2cfdde6bd"
     }
    },
    "4f0711e5af1549ada63f9b2bfbbbbe3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5482591f3a4f4db5a9a9fdd1528426a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "591f613cf9d34e01a59ff7f56d981bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "829e0e0b096a4f72935dfe636bc03e06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_932330cf200746d79a3b902c568ad21e",
      "placeholder": "​",
      "style": "IPY_MODEL_275321de10444260afab1c77794dc44d",
      "value": " 1.45M/1.45M [00:00&lt;00:00, 18.3MB/s]"
     }
    },
    "932330cf200746d79a3b902c568ad21e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b88579fe4e0243239ac90b917b7ddfc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c046f8c23c294c7b892af4c47f9a4bd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db20b4846da74816ab0200ff2266b9e0",
       "IPY_MODEL_112ab4a81f0a47a4bbfce242f9e00b95",
       "IPY_MODEL_829e0e0b096a4f72935dfe636bc03e06"
      ],
      "layout": "IPY_MODEL_4f0711e5af1549ada63f9b2bfbbbbe3b"
     }
    },
    "c1575672ebb04753bb69e0cd533bebcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c5e46ace07864489bbd5e7f85461a171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da42b769e9c64de2bbab7abb7de2447b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1575672ebb04753bb69e0cd533bebcb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_591f613cf9d34e01a59ff7f56d981bcc",
      "value": 1
     }
    },
    "db20b4846da74816ab0200ff2266b9e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ede39c58e797410fab84475756648370",
      "placeholder": "​",
      "style": "IPY_MODEL_3a446921402c4497938b3ba9556b023e",
      "value": "Downloading data: 100%"
     }
    },
    "e14d259844f0485e853af4e2cfdde6bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ede39c58e797410fab84475756648370": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f21ab62168f440178c30cb9d0cb310f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f349749a5ba54b34ab5311471634d7dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b88579fe4e0243239ac90b917b7ddfc9",
      "placeholder": "​",
      "style": "IPY_MODEL_2925a382b1a04f5b988c815bd406e935",
      "value": " 7012/0 [00:03&lt;00:00, 1741.87 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
